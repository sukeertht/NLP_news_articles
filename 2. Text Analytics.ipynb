{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "## Increasing the width of the output display\n",
    "pd.set_option('display.max_colwidth',100) \n",
    "\n",
    "## setting the file path\n",
    "file=r\" \"\n",
    "\n",
    "## reading the csv file\n",
    "data=pd.read_csv(file, engine='python')\n",
    "\n",
    "##Initializing Stopwords, Stemmer and Lemmatizer functions\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "# stop = ['said','would']\n",
    "\n",
    "##Create function to remove punctuation, remove numbers, tokenize, remove stopwords and lemmatize\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    num = re.sub('[0-9]+','',text)\n",
    "    text = \"\".join([word for word in text if word in num])\n",
    "    text = \" \".join([word for word in text.split() if len(word)>2])\n",
    "    ##text = \" \".join([word for word in text.split() if word not in stop])\n",
    "    tokens = re.split('\\W+',text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['News_2'] = data['News'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization - Convert to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Apply CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer = clean_text)\n",
    "X_counts = count_vect.fit_transform(data['News'])\n",
    "print(X_counts.shape)\n",
    "\n",
    "## Create Sparse matrix\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "X_counts_df.head()\n",
    "# To know frequency of specific word\n",
    "# X_counts_df.facebook.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summation of word frequencies\n",
    "sum_words = X_counts.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "## Select the top ‘X’ frequency words (eg: here X=10)\n",
    "dat = words_freq[:10]\n",
    "\n",
    "dat_df=pd.DataFrame(dat)\n",
    "dat_df.columns=['word','freq']\n",
    "\n",
    "## Plot the top frequency words\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.bar(dat_df['word'],dat_df['freq'])\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"wordfreq.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "## Print the top 'X' frequency words\n",
    "# dat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##abc=\"said\",\"mr\",\"company\",\"deal\"\n",
    "# stopwords = set(abc)\n",
    "##wordcloud = WordCloud(background_color='white',stopwords=stopwords).generate(str(words_freq))\n",
    "wordcloud = WordCloud(max_words=100,background_color='white',width=600,height=300).generate(str(words_freq))\n",
    "plt.figure(figsize=(6,2),dpi=200)\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"wordcloud.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis using TextBlob\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "\n",
    "bloblist_desc = list()\n",
    "\n",
    "news_str = data['News_2'].astype(str)\n",
    "for row in news_str:\n",
    "    blob = TextBlob(row)\n",
    "    bloblist_desc.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))\n",
    "    polarity_desc = pd.DataFrame(bloblist_desc, columns = ['sentence','polarity','subjectivity'])\n",
    "\n",
    "def f(polarity_desc):\n",
    "    if polarity_desc['polarity'] > 0:\n",
    "        val = \"Positive\"\n",
    "    elif polarity_desc['polarity'] == 0:\n",
    "        val = \"Neutral\"\n",
    "    else:\n",
    "        val = \"Negative\"\n",
    "    return val\n",
    "\n",
    "polarity_desc['Sentiment_Type'] = polarity_desc.apply(f, axis = 1)\n",
    "\n",
    "## Create sentiment scores csv file\n",
    "df1 = pd.DataFrame(polarity_desc)\n",
    "my_df2 = pd.DataFrame(data)\n",
    "df2 = pd.DataFrame(my_df2.iloc[:,:4])\n",
    "df = pd.concat([df2,df1],axis = 1)\n",
    "df.to_csv(\"scores.csv\",index=False)\n",
    "\n",
    "## Plot sentiment scores\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.countplot(x = \"Sentiment_Type\", data = polarity_desc)\n",
    "plt.savefig('sentiment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling__Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(data['News_2'])\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in data['News_2']]\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#num_topics indicates number of topics\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=5, random_state=100,\n",
    "                chunksize=1000, passes=5)\n",
    "\n",
    "# lda_model.print_topics()\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda.html')\n",
    "pyLDAvis.show(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
